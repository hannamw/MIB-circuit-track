{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnsight import LanguageModel\n",
    "# model = LanguageModel('gpt2-small', device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from graph import Graph\n",
    "from attribute import attribute\n",
    "from dataset import HFEAPDataset\n",
    "from metrics import get_metric\n",
    "from evaluate_graph import evaluate_graph, evaluate_baseline, evaluate_area_under_curve\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from typing import Callable, List, Union, Optional, Literal\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_attention_mask\n",
    "from tqdm import tqdm\n",
    "from einops import einsum\n",
    "\n",
    "from graph import Graph, InputNode, LogitNode, AttentionNode, MLPNode\n",
    "\n",
    "\n",
    "def to_tokens(tokenizer, model, input_text, prepend_bos=True, padding_side='right', move_to_device=True, truncate=True, max_length=None):\n",
    "    \"\"\"\n",
    "    Converts input text to tokens using HuggingFace tokenizer with similar functionality to TransformerLens.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        model: HuggingFace model (for device information if move_to_device=True)\n",
    "        input_text (Union[str, List[str]]): The input to tokenize\n",
    "        prepend_bos (bool, optional): Whether to prepend the BOS token. Defaults to True.\n",
    "        padding_side (str, optional): Side to pad on ('left' or 'right'). Defaults to 'right'.\n",
    "        move_to_device (bool, optional): Whether to move tensors to model's device. Defaults to True.\n",
    "        truncate (bool, optional): Whether to truncate to model's max length. Defaults to True.\n",
    "        max_length (int, optional): Maximum length to truncate to. If None, uses model's max length.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of token ids\n",
    "    \"\"\"\n",
    "    # Save original padding side\n",
    "    original_padding_side = tokenizer.padding_side\n",
    "    tokenizer.padding_side = padding_side\n",
    "    \n",
    "    # Handle BOS token\n",
    "    add_special_tokens = prepend_bos\n",
    "    \n",
    "    # Determine max_length for truncation\n",
    "    if truncate and max_length is None:\n",
    "        if hasattr(model.config, 'max_position_embeddings'):\n",
    "            max_length = model.config.max_position_embeddings\n",
    "        else:\n",
    "            max_length = model.config.n_positions  # for GPT-2\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        input_text,\n",
    "        add_special_tokens=add_special_tokens,  # This handles BOS token if model has one\n",
    "        padding=True if isinstance(input_text, list) else False,  # Only pad for batch inputs\n",
    "        truncation=truncate,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device if requested\n",
    "    if move_to_device and hasattr(model, 'device'):\n",
    "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    # Restore original padding side\n",
    "    tokenizer.padding_side = original_padding_side\n",
    "    \n",
    "    return tokens['input_ids']\n",
    "\n",
    "def tokenize_plus_nnsight(model: HookedTransformer, inputs: List[str], max_length: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Tokenizes the input strings using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The model used for tokenization.\n",
    "        inputs (List[str]): The list of input strings to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - tokens (torch.Tensor): The tokenized inputs.\n",
    "            - attention_mask (torch.Tensor): The attention mask for the tokenized inputs.\n",
    "            - input_lengths (torch.Tensor): The lengths of the tokenized inputs.\n",
    "            - n_pos (int): The maximum sequence length of the tokenized inputs.\n",
    "    \"\"\"\n",
    "    if max_length is not None:\n",
    "        old_n_ctx = model.config.n_ctx\n",
    "        model.config.n_ctx = max_length\n",
    "\n",
    "\n",
    "    # tokens = model.to_tokens(inputs, prepend_bos=True, padding_side='right', truncate=(max_length is not None))\n",
    "    # Shun's change\n",
    "    tokenizer = model.tokenizer\n",
    "    tokens = to_tokens(tokenizer, model, inputs, prepend_bos=True, padding_side='right', truncate=(max_length is not None))\n",
    "    \n",
    "    \n",
    "    if max_length is not None:\n",
    "        model.config.n_ctx = old_n_ctx\n",
    "    attention_mask = get_attention_mask(model.tokenizer, tokens, True)\n",
    "    input_lengths = attention_mask.sum(1)\n",
    "    n_pos = attention_mask.size(1)\n",
    "    return tokens, attention_mask, input_lengths, n_pos\n",
    "\n",
    "\n",
    "def make_hooks_and_matrices(model: HookedTransformer, graph: Graph, batch_size:int , n_pos:int, scores: Optional[Tensor]):\n",
    "    \"\"\"Makes a matrix, and hooks to fill it and the score matrix up\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): model to attribute\n",
    "        graph (Graph): graph to attribute\n",
    "        batch_size (int): size of the particular batch you're attributing\n",
    "        n_pos (int): size of the position dimension\n",
    "        scores (Tensor): The scores tensor you intend to fill. If you pass in None, we assume that you're using these hooks / matrices for evaluation only (so don't use the backwards hooks!)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[List, List, List], Tensor]: The final tensor ([batch, pos, n_src_nodes, d_model]) stores activation differences, i.e. corrupted - clean activations. The first set of hooks will add in the activations they are run on (run these on corrupted input), while the second set will subtract out the activations they are run on (run these on clean input). The third set of hooks will compute the gradients and update the scores matrix that you passed in. \n",
    "    \"\"\"\n",
    "    separate_activations = model.config.use_normalization_before_and_after and scores is None\n",
    "    if separate_activations:\n",
    "        activation_difference = torch.zeros((2, batch_size, n_pos, graph.n_forward, model.config.d_model), device=model.config.device, dtype=model.config.dtype)\n",
    "    else:\n",
    "        # activation_difference = torch.zeros((batch_size, n_pos, graph.n_forward, model.config.d_model), device=model.config.device, dtype=model.config.dtype)\n",
    "        activation_difference = torch.zeros(\n",
    "            (batch_size, n_pos, graph.n_forward, model.config.n_embd),  # using n_embd instead of d_model\n",
    "            device=model.device,  # device from model instead of config\n",
    "            dtype=model.dtype    # dtype from model instead of config\n",
    "        )\n",
    "\n",
    "    processed_attn_layers = set()\n",
    "    fwd_hooks_clean = []\n",
    "    fwd_hooks_corrupted = []\n",
    "    bwd_hooks = []\n",
    "        \n",
    "    # Fills up the activation difference matrix. In the default case (not separate_activations), \n",
    "    # we add in the corrupted activations (add = True) and subtract out the clean ones (add=False)\n",
    "    # In the separate_activations case, we just store them in two halves of the matrix. Less efficient, \n",
    "    # but necessary for models with Gemma's architecture.\n",
    "    def activation_hook(index, activations, hook, add:bool=True):\n",
    "        acts = activations.detach()\n",
    "        try:\n",
    "            if separate_activations:\n",
    "                if add:\n",
    "                    activation_difference[0, :, :, index] += acts\n",
    "                else:\n",
    "                    activation_difference[1, :, :, index] += acts\n",
    "            else:\n",
    "                if add:\n",
    "                    activation_difference[:, :, index] += acts\n",
    "                else:\n",
    "                    activation_difference[:, :, index] -= acts\n",
    "        except RuntimeError as e:\n",
    "            print(hook.name, activation_difference[:, :, index].size(), acts.size())\n",
    "            raise e\n",
    "    \n",
    "    def gradient_hook(prev_index: int, bwd_index: Union[slice, int], gradients:torch.Tensor, hook):\n",
    "        \"\"\"Takes in a gradient and uses it and activation_difference \n",
    "        to compute an update to the score matrix\n",
    "\n",
    "        Args:\n",
    "            fwd_index (Union[slice, int]): The forward index of the (src) node\n",
    "            bwd_index (Union[slice, int]): The backward index of the (dst) node\n",
    "            gradients (torch.Tensor): The gradients of this backward pass \n",
    "            hook (_type_): (unused)\n",
    "\n",
    "        \"\"\"\n",
    "        grads = gradients.detach()\n",
    "        try:\n",
    "            if grads.ndim == 3:\n",
    "                grads = grads.unsqueeze(2)\n",
    "            s = einsum(activation_difference[:, :, :prev_index], grads,'batch pos forward hidden, batch pos backward hidden -> forward backward')\n",
    "            s = s.squeeze(1)\n",
    "            scores[:prev_index, bwd_index] += s\n",
    "        except RuntimeError as e:\n",
    "            print(hook.name, activation_difference.size(), activation_difference.device, grads.size(), grads.device)\n",
    "            print(prev_index, bwd_index, scores.size(), s.size())\n",
    "            raise e\n",
    "    \n",
    "    node = graph.nodes['input']\n",
    "    fwd_index = graph.forward_index(node)\n",
    "    fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "    fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "    \n",
    "    for layer in range(graph.cfg['n_layers']):\n",
    "        node = graph.nodes[f'a{layer}.h0']\n",
    "        fwd_index = graph.forward_index(node)\n",
    "        fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "        fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "        prev_index = graph.prev_index(node)\n",
    "        for i, letter in enumerate('qkv'):\n",
    "            bwd_index = graph.backward_index(node, qkv=letter)\n",
    "            bwd_hooks.append((node.qkv_inputs[i], partial(gradient_hook, prev_index, bwd_index)))\n",
    "\n",
    "        node = graph.nodes[f'm{layer}']\n",
    "        fwd_index = graph.forward_index(node)\n",
    "        bwd_index = graph.backward_index(node)\n",
    "        prev_index = graph.prev_index(node)\n",
    "        fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "        fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "        bwd_hooks.append((node.in_hook, partial(gradient_hook, prev_index, bwd_index)))\n",
    "        \n",
    "    node = graph.nodes['logits']\n",
    "    prev_index = graph.prev_index(node)\n",
    "    bwd_index = graph.backward_index(node)\n",
    "    bwd_hooks.append((node.in_hook, partial(gradient_hook, prev_index, bwd_index)))\n",
    "            \n",
    "    return (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks), activation_difference\n",
    "\n",
    "\n",
    "# def get_scores_eap_nnsight(model: HookedTransformer, graph: Graph, dataloader:DataLoader, metric: Callable[[Tensor], Tensor], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "#     \"\"\"Gets edge attribution scores using EAP.\n",
    "\n",
    "#     Args:\n",
    "#         model (HookedTransformer): The model to attribute\n",
    "#         graph (Graph): Graph to attribute\n",
    "#         dataloader (DataLoader): The data over which to attribute\n",
    "#         metric (Callable[[Tensor], Tensor]): metric to attribute with respect to\n",
    "#         quiet (bool, optional): suppress tqdm output. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: a [src_nodes, dst_nodes] tensor of scores for each edge\n",
    "#     \"\"\"\n",
    "#     # scores = torch.zeros((graph.n_forward, graph.n_backward), device='cuda', dtype=model.config.dtype)\n",
    "#     scores = torch.zeros((graph.n_forward, graph.n_backward), device='cpu', dtype=model.config.dtype)\n",
    "\n",
    "#     if 'mean' in intervention:\n",
    "#         assert intervention_dataloader is not None, \"Intervention dataloader must be provided for mean interventions\"\n",
    "    #     per_position = 'positional' in intervention\n",
    "    #     means = compute_mean_activations(model, graph, intervention_dataloader, per_position=per_position)\n",
    "    #     means = means.unsqueeze(0)\n",
    "    #     if not per_position:\n",
    "    #         means = means.unsqueeze(0)\n",
    "    \n",
    "    # total_items = 0\n",
    "    # dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    # for clean, corrupted, label in dataloader:\n",
    "    #     batch_size = len(clean)\n",
    "    #     total_items += batch_size\n",
    "    #     clean_tokens, attention_mask, input_lengths, n_pos = tokenize_plus_nnsight(model, clean)\n",
    "    #     corrupted_tokens, _, _, _ = tokenize_plus_nnsight(model, corrupted)\n",
    "\n",
    "    #     (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks), activation_difference = make_hooks_and_matrices(model, graph, batch_size, n_pos, scores)\n",
    "\n",
    "    #     with torch.inference_mode():\n",
    "    #         if intervention == 'patching':\n",
    "    #             # We intervene by subtracting out clean and adding in corrupted activations\n",
    "    #             with model.hooks(fwd_hooks_corrupted):\n",
    "    #                 _ = model(corrupted_tokens, attention_mask=attention_mask)\n",
    "                    \n",
    "                    \n",
    "    #         elif 'mean' in intervention:\n",
    "    #             # In the case of zero or mean ablation, we skip the adding in corrupted activations\n",
    "    #             # but in mean ablations, we need to add the mean in\n",
    "    #             activation_difference += means\n",
    "\n",
    "    #         # For some metrics (e.g. accuracy or KL), we need the clean logits\n",
    "    #         clean_logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "\n",
    "    #     with model.hooks(fwd_hooks=fwd_hooks_clean, bwd_hooks=bwd_hooks):\n",
    "    #         logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "    #         metric_value = metric(logits, clean_logits, input_lengths, label)\n",
    "    #         metric_value.backward()\n",
    "\n",
    "    # scores /= total_items\n",
    "\n",
    "    # return scores\n",
    "    \n",
    "\n",
    "allowed_aggregations = {'sum', 'mean'}#, 'l2'}        \n",
    "def attribute_nnsight(model: LanguageModel, graph: Graph, dataloader: DataLoader, metric: Callable[[Tensor], Tensor], method: Literal['EAP', 'EAP-IG-inputs', 'clean-corrupted', 'EAP-IG-activations'], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', aggregation='sum', ig_steps: Optional[int]=None, intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "    assert model.config.use_attn_result, \"Model must be configured to use attention result (model.config.use_attn_result)\"\n",
    "    assert model.config.use_split_qkv_input, \"Model must be configured to use split qkv inputs (model.config.use_split_qkv_input)\"\n",
    "    assert model.config.use_hook_mlp_in, \"Model must be configured to use hook MLP in (model.config.use_hook_mlp_in)\"\n",
    "    if model.config.n_key_value_heads is not None:\n",
    "        assert model.config.ungroup_grouped_query_attention, \"Model must be configured to ungroup grouped attention (model.config.ungroup_grouped_query_attention = True)\"\n",
    "    \n",
    "    if aggregation not in allowed_aggregations:\n",
    "        raise ValueError(f'aggregation must be in {allowed_aggregations}, but got {aggregation}')\n",
    "        \n",
    "    # Scores are by default summed across the d_model dimension\n",
    "    # This means that scores are a [n_src_nodes, n_dst_nodes] tensor\n",
    "    if method == 'EAP':\n",
    "        scores = get_scores_eap_nnsight(model, graph, dataloader, metric, intervention=intervention, intervention_dataloader=intervention_dataloader, quiet=quiet)\n",
    "    elif method == 'EAP-IG-inputs':\n",
    "        if intervention != 'patching':\n",
    "            raise ValueError(f\"intervention must be 'patching' for EAP-IG-inputs, but got {intervention}\")\n",
    "        scores = get_scores_eap_ig(model, graph, dataloader, metric, steps=ig_steps, quiet=quiet)\n",
    "    elif method == 'clean-corrupted':\n",
    "        if intervention != 'patching':\n",
    "            raise ValueError(f\"intervention must be 'patching' for clean-corrupted, but got {intervention}\")\n",
    "        scores = get_scores_clean_corrupted(model, graph, dataloader, metric, quiet=quiet)\n",
    "    elif method == 'EAP-IG-activations':\n",
    "        scores = get_scores_ig_activations(model, graph, dataloader, metric, steps=ig_steps, intervention=intervention, intervention_dataloader=intervention_dataloader, quiet=quiet)\n",
    "    else:\n",
    "        raise ValueError(f\"integrated_gradients must be in ['EAP', 'EAP-IG-inputs', 'EAP-IG-activations'], but got {method}\")\n",
    "\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        scores /= model.config.d_model\n",
    "        \n",
    "    graph.scores[:] =  scores.to(graph.scores.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_eap_qkv(model: LanguageModel, graph: Graph, dataloader: DataLoader, metric: Callable[[Tensor], Tensor], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "    scores = torch.zeros((graph.n_forward, graph.n_backward), device='cpu', dtype=model.dtype)\n",
    "\n",
    "    if 'mean' in intervention:\n",
    "        assert intervention_dataloader is not None\n",
    "        per_position = 'positional' in intervention\n",
    "        means = compute_mean_activations(model, graph, intervention_dataloader, per_position=per_position)\n",
    "        means = means.unsqueeze(0)\n",
    "        if not per_position:\n",
    "            means = means.unsqueeze(0)\n",
    "    \n",
    "    total_items = 0\n",
    "    dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    for clean, corrupted, label in dataloader:\n",
    "        batch_size = len(clean)\n",
    "        total_items += batch_size\n",
    "        clean_tokens, attention_mask, input_lengths, n_pos = tokenize_plus_nnsight(model, clean)\n",
    "        corrupted_tokens, _, _, _ = tokenize_plus_nnsight(model, corrupted)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if intervention == 'patching':\n",
    "                # Initialize activation differences tensor\n",
    "                activation_difference = torch.zeros(\n",
    "                    (batch_size, n_pos, graph.n_forward, model.config.hidden_size), \n",
    "                    device=model.device, \n",
    "                    dtype=model.dtype\n",
    "                )\n",
    "\n",
    "                # Get corrupted activations\n",
    "                with model.trace({\"input_ids\": corrupted_tokens, \"attention_mask\": attention_mask}):\n",
    "                    for layer in range(graph.cfg['n_layers']):\n",
    "                        #### Attention #########\n",
    "                        node = graph.nodes[f'a{layer}.h0']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        attn_hs = model.transformer.h[layer].attn.c_proj.input\n",
    "\n",
    "                        by_head = split_heads_nns(attn_hs, model.config.n_head, model.config.hidden_size)\n",
    "                        by_head = model.transformer.h[layer].attn.c_proj(by_head)\n",
    "                        by_head = model.transformer.h[layer].attn.resid_dropout(by_head)\n",
    "                        \n",
    "                        activation_difference[:, :, fwd_index] += by_head\n",
    "\n",
    "                        #### MLP #########\n",
    "                        node = graph.nodes[f'm{layer}']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        activation_difference[:, :, fwd_index][:] += model.transformer.h[layer].mlp.output[:]\n",
    "\n",
    "            elif 'mean' in intervention:\n",
    "                activation_difference += means\n",
    "\n",
    "            # Get clean logits for reference\n",
    "            clean_logits = model.trace({'input_ids': clean_tokens, 'attention_mask':attention_mask}, trace=False)['logits']\n",
    "\n",
    "            # Run with interventions\n",
    "            with model.trace({\"input_ids\": clean_tokens, \"attention_mask\": attention_mask}):\n",
    "                for layer in range(graph.cfg['n_layers']):\n",
    "                    if any(graph.nodes[f'a{layer}.h{head}'].in_graph for head in range(model.config.n_head)):\n",
    "                        # Handle QKV inputs\n",
    "                        qkv_inp = model.transformer.h[layer].ln_1.input\n",
    "                        c_proj_out = []\n",
    "\n",
    "                        for ii, letter in enumerate('qkv'):\n",
    "                            node = graph.nodes[f'a{layer}.h0']\n",
    "                            prev_index = graph.prev_index(node)\n",
    "                            bwd_index = graph.backward_index(node, qkv=letter, attn_slice=True)\n",
    "\n",
    "                            update = einsum(activation_difference[:, :, :len(in_graph_matrix[:prev_index, bwd_index])], \n",
    "                                         in_graph_matrix[:prev_index, bwd_index],\n",
    "                                         'batch pos previous hidden, previous ... -> batch pos ... hidden')\n",
    "\n",
    "                            update = update.to(torch.float16)\n",
    "\n",
    "                            qkv_out = []\n",
    "                            for head in range(model.config.n_head):\n",
    "                                qkv_in_clone = qkv_inp.clone()\n",
    "                                update_head = update[:, :, head, :]\n",
    "                                qkv_in_clone += update_head\n",
    "\n",
    "                                qkv_in_clone = model.transformer.h[layer].ln_1(qkv_in_clone)\n",
    "                                attn_out = model.transformer.h[layer].attn.c_attn(qkv_in_clone)\n",
    "\n",
    "                                start = ii * head * (model.config.hidden_size // model.config.n_head)\n",
    "                                end = start + (model.config.hidden_size // model.config.n_head)\n",
    "                                attn_head_out = attn_out[:, :, start:end]\n",
    "                                \n",
    "                                qkv_out.append(attn_head_out)\n",
    "                            \n",
    "                            qkv_out = torch.cat(qkv_out, dim=-1)\n",
    "                            c_proj_out.append(qkv_out)\n",
    "\n",
    "                        c_proj_out = torch.cat(c_proj_out, dim=-1)\n",
    "                        model.transformer.h[layer].attn.c_attn.output = c_proj_out\n",
    "\n",
    "                    # MLP handling\n",
    "                    if graph.nodes[f'm{layer}'].in_graph:\n",
    "                        node = graph.nodes[f'm{layer}']\n",
    "                        prev_index = graph.prev_index(node)\n",
    "                        bwd_index = graph.backward_index(node)\n",
    "\n",
    "                        update = einsum(activation_difference[:, :, :len(in_graph_matrix[:prev_index, bwd_index])], \n",
    "                                     in_graph_matrix[:prev_index, bwd_index],\n",
    "                                     'batch pos previous hidden, previous ... -> batch pos ... hidden')\n",
    "                        \n",
    "                        model.transformer.h[layer].ln_2.input += update\n",
    "\n",
    "                logits = model.lm_head.output.save()\n",
    "\n",
    "                metric_value = metric(logits, clean_logits, input_lengths, label)\n",
    "                metric_value.backward()\n",
    "\n",
    "                # Collect gradients and update scores\n",
    "                grads = model.lm_head.output.grad\n",
    "                s = einsum(activation_difference, grads,\n",
    "                          'batch pos forward hidden, batch pos token -> forward')\n",
    "                scores += s\n",
    "\n",
    "    scores /= total_items\n",
    "    return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scores_eap_nnsight(model: LanguageModel, graph: Graph, dataloader: DataLoader, metric: Callable[[Tensor], Tensor], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "    \"\"\"Gets edge attribution scores using EAP.\n",
    "    Args:\n",
    "        model (LanguageModel): The model to attribute\n",
    "        graph (Graph): Graph to attribute\n",
    "        dataloader (DataLoader): The data over which to attribute\n",
    "        metric (Callable[[Tensor], Tensor]): metric to attribute with respect to\n",
    "        intervention (str): Type of intervention ('patching', 'zero', 'mean', 'mean-positional')\n",
    "        intervention_dataloader (Optional[DataLoader]): Dataset for mean computation\n",
    "        quiet (bool): Whether to suppress progress bar\n",
    "    Returns:\n",
    "        Tensor: a [src_nodes, dst_nodes] tensor of scores for each edge\n",
    "    \"\"\"\n",
    "    scores = torch.zeros((graph.n_forward, graph.n_backward), device='cpu', dtype=model.dtype)\n",
    "\n",
    "    if 'mean' in intervention:\n",
    "        assert intervention_dataloader is not None, \"Intervention dataloader must be provided for mean interventions\"\n",
    "        per_position = 'positional' in intervention\n",
    "        means = compute_mean_activations(model, graph, intervention_dataloader, per_position=per_position)\n",
    "        means = means.unsqueeze(0)\n",
    "        if not per_position:\n",
    "            means = means.unsqueeze(0)\n",
    "    \n",
    "    total_items = 0\n",
    "    dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    for clean, corrupted, label in dataloader:\n",
    "        batch_size = len(clean)\n",
    "        total_items += batch_size\n",
    "        clean_tokens, attention_mask, input_lengths, n_pos = tokenize_plus_nnsight(model, clean)\n",
    "        corrupted_tokens, _, _, _ = tokenize_plus_nnsight(model, corrupted)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if intervention == 'patching':\n",
    "                # Initialize activation differences tensor\n",
    "                activation_difference = torch.zeros(\n",
    "                    (batch_size, n_pos, graph.n_forward, model.config.n_embd), \n",
    "                    device=model.device, \n",
    "                    dtype=model.dtype\n",
    "                )\n",
    "\n",
    "                # Capture corrupted activations\n",
    "                with model.trace({\"input_ids\": corrupted_tokens, \"attention_mask\": attention_mask}):\n",
    "                    for layer in range(graph.cfg['n_layers']):\n",
    "                        # Attention handling\n",
    "                        node = graph.nodes[f'a{layer}.h0']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        attn_hs = model.transformer.h[layer].attn.c_proj.input\n",
    "\n",
    "                        by_head = split_heads_nns(attn_hs, model.config.n_head, model.config.n_embd)\n",
    "                        by_head = model.transformer.h[layer].attn.c_proj(by_head)\n",
    "                        by_head = model.transformer.h[layer].attn.resid_dropout(by_head)\n",
    "                        \n",
    "                        activation_difference[:, :, fwd_index] += by_head\n",
    "\n",
    "                        # MLP handling\n",
    "                        node = graph.nodes[f'm{layer}']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        activation_difference[:, :, fwd_index][:] += model.transformer.h[layer].mlp.output[:]\n",
    "\n",
    "            elif 'mean' in intervention:\n",
    "                activation_difference += means\n",
    "\n",
    "            # Get clean logits for reference\n",
    "            with model.trace({\"input_ids\": clean_tokens, \"attention_mask\": attention_mask}):\n",
    "                clean_logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "\n",
    "            # Run with activation differences injected\n",
    "            with model.trace({\"input_ids\": clean_tokens, \"attention_mask\": attention_mask}) as trace:\n",
    "                for layer in range(graph.cfg['n_layers']):\n",
    "                    # Inject attention differences\n",
    "                    node = graph.nodes[f'a{layer}.h0']\n",
    "                    fwd_index = graph.forward_index(node)\n",
    "                    model.transformer.h[layer].attn.output[:] += activation_difference[:, :, fwd_index]\n",
    "\n",
    "                    # Inject MLP differences\n",
    "                    node = graph.nodes[f'm{layer}']\n",
    "                    fwd_index = graph.forward_index(node)\n",
    "                    model.transformer.h[layer].mlp.output[:] += activation_difference[:, :, fwd_index]\n",
    "\n",
    "                logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "                metric_value = metric(logits, clean_logits, input_lengths, label)\n",
    "                metric_value.backward()\n",
    "\n",
    "                # Collect gradients and update scores\n",
    "                for layer in range(graph.cfg['n_layers']):\n",
    "                    attn_grads = model.transformer.h[layer].attn.output.grad\n",
    "                    mlp_grads = model.transformer.h[layer].mlp.output.grad\n",
    "\n",
    "                    if attn_grads.ndim == 3:\n",
    "                        attn_grads = attn_grads.unsqueeze(2)\n",
    "                    if mlp_grads.ndim == 3:\n",
    "                        mlp_grads = mlp_grads.unsqueeze(2)\n",
    "\n",
    "                    # Update attention scores\n",
    "                    node = graph.nodes[f'a{layer}.h0']\n",
    "                    fwd_index = graph.forward_index(node)\n",
    "                    s_attn = einsum(activation_difference[:, :, :fwd_index], attn_grads,\n",
    "                                  'batch pos forward hidden, batch pos backward hidden -> forward backward')\n",
    "                    scores[:fwd_index] += s_attn.squeeze(1)\n",
    "\n",
    "                    # Update MLP scores\n",
    "                    node = graph.nodes[f'm{layer}']\n",
    "                    fwd_index = graph.forward_index(node)\n",
    "                    s_mlp = einsum(activation_difference[:, :, :fwd_index], mlp_grads,\n",
    "                                 'batch pos forward hidden, batch pos backward hidden -> forward backward')\n",
    "                    scores[:fwd_index] += s_mlp.squeeze(1)\n",
    "\n",
    "    scores /= total_items\n",
    "    return scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('gpt2', device_map='cpu')\n",
    "model.config.use_split_qkv_input = True\n",
    "model.config.use_attn_result = True\n",
    "model.config.use_hook_mlp_in = True\n",
    "model.config.ungroup_grouped_query_attention = True\n",
    "\n",
    "dataset = HFEAPDataset(\"danaarad/ioi_dataset\", model.tokenizer, task=\"ioi\", num_examples=100)\n",
    "dataloader = dataset.to_dataloader(20)\n",
    "metric_fn = get_metric(\"logit_diff\", \"ioi\", model.tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.47.0\",\n",
       "  \"ungroup_grouped_query_attention\": true,\n",
       "  \"use_attn_result\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"use_hook_mlp_in\": true,\n",
       "  \"use_split_qkv_input\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.n_key_value_heads = None\n",
    "model.config.dtype = torch.float32\n",
    "model.config.use_normalization_before_and_after = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph.from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      " 0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'GPT2LMHeadModel' object has no attribute 'hooks'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mattribute_nnsight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEAP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 217\u001b[0m, in \u001b[0;36mattribute_nnsight\u001b[0;34m(model, graph, dataloader, metric, method, intervention, aggregation, ig_steps, intervention_dataloader, quiet)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;66;03m# Scores are by default summed across the d_model dimension\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This means that scores are a [n_src_nodes, n_dst_nodes] tensor\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEAP\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_scores_eap_nnsight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEAP-IG-inputs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intervention \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatching\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[5], line 181\u001b[0m, in \u001b[0;36mget_scores_eap_nnsight\u001b[0;34m(model, graph, dataloader, metric, intervention, intervention_dataloader, quiet)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intervention \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatching\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;66;03m# We intervene by subtracting out clean and adding in corrupted activations\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhooks\u001b[49m(fwd_hooks_corrupted):\n\u001b[1;32m    182\u001b[0m             _ \u001b[38;5;241m=\u001b[39m model(corrupted_tokens, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m intervention:\n\u001b[1;32m    186\u001b[0m         \u001b[38;5;66;03m# In the case of zero or mean ablation, we skip the adding in corrupted activations\u001b[39;00m\n\u001b[1;32m    187\u001b[0m         \u001b[38;5;66;03m# but in mean ablations, we need to add the mean in\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:540\u001b[0m, in \u001b[0;36mNNsight.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, InterventionProxy, Any]:\n\u001b[1;32m    535\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper of ._envoy's attributes to access module's inputs and outputs.\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \n\u001b[1;32m    537\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_envoy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/envoy.py:400\u001b[0m, in \u001b[0;36mEnvoy.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Envoy, Any]:\n\u001b[1;32m    391\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper method for underlying module's attributes.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \n\u001b[1;32m    393\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;124;03m        Any: Attribute.\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 400\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GPT2LMHeadModel' object has no attribute 'hooks'"
     ]
    }
   ],
   "source": [
    "attribute_nnsight(model, g, dataloader, partial(metric_fn, loss=True, mean=True), 'EAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight: torch.float32\n",
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Print the dtype of model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "    # You can break after first parameter since they typically share the same dtype\n",
    "    break\n",
    "\n",
    "# Alternative: get dtype of the first parameter directly\n",
    "first_param = next(model.parameters())\n",
    "print(f\"Model dtype: {first_param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
