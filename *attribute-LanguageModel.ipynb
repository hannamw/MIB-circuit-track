{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nnsight import LanguageModel\n",
    "# model = LanguageModel('gpt2-small', device_map='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/opt/anaconda3/envs/mib/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/runpy.py\", line 87, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/asyncio/base_events.py\", line 601, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/asyncio/base_events.py\", line 1905, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3048, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3103, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3308, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3490, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3550, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/v7/4qsbb8t11hg55cv265kmzw8r0000gn/T/ipykernel_65744/205138925.py\", line 6, in <module>\n",
      "    import torch\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/mib/lib/python3.9/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/opt/anaconda3/envs/mib/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from graph import Graph\n",
    "from attribute import attribute\n",
    "from dataset import HFEAPDataset\n",
    "from metrics import get_metric\n",
    "from evaluate_graph import evaluate_graph, evaluate_baseline, evaluate_area_under_curve\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "from typing import Callable, List, Union, Optional, Literal\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import Tensor\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.utils import get_attention_mask\n",
    "from tqdm import tqdm\n",
    "from einops import einsum\n",
    "\n",
    "from graph import Graph, InputNode, LogitNode, AttentionNode, MLPNode\n",
    "\n",
    "\n",
    "def to_tokens(tokenizer, model, input_text, prepend_bos=True, padding_side='right', move_to_device=True, truncate=True, max_length=None):\n",
    "    \"\"\"\n",
    "    Converts input text to tokens using HuggingFace tokenizer with similar functionality to TransformerLens.\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace tokenizer\n",
    "        model: HuggingFace model (for device information if move_to_device=True)\n",
    "        input_text (Union[str, List[str]]): The input to tokenize\n",
    "        prepend_bos (bool, optional): Whether to prepend the BOS token. Defaults to True.\n",
    "        padding_side (str, optional): Side to pad on ('left' or 'right'). Defaults to 'right'.\n",
    "        move_to_device (bool, optional): Whether to move tensors to model's device. Defaults to True.\n",
    "        truncate (bool, optional): Whether to truncate to model's max length. Defaults to True.\n",
    "        max_length (int, optional): Maximum length to truncate to. If None, uses model's max length.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of token ids\n",
    "    \"\"\"\n",
    "    # Save original padding side\n",
    "    original_padding_side = tokenizer.padding_side\n",
    "    tokenizer.padding_side = padding_side\n",
    "    \n",
    "    # Handle BOS token\n",
    "    add_special_tokens = prepend_bos\n",
    "    \n",
    "    # Determine max_length for truncation\n",
    "    if truncate and max_length is None:\n",
    "        if hasattr(model.config, 'max_position_embeddings'):\n",
    "            max_length = model.config.max_position_embeddings\n",
    "        else:\n",
    "            max_length = model.config.n_positions  # for GPT-2\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(\n",
    "        input_text,\n",
    "        add_special_tokens=add_special_tokens,  # This handles BOS token if model has one\n",
    "        padding=True if isinstance(input_text, list) else False,  # Only pad for batch inputs\n",
    "        truncation=truncate,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move to device if requested\n",
    "    if move_to_device and hasattr(model, 'device'):\n",
    "        tokens = {k: v.to(model.device) for k, v in tokens.items()}\n",
    "    \n",
    "    # Restore original padding side\n",
    "    tokenizer.padding_side = original_padding_side\n",
    "    \n",
    "    return tokens['input_ids']\n",
    "\n",
    "def tokenize_plus_nnsight(model: HookedTransformer, inputs: List[str], max_length: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Tokenizes the input strings using the provided model.\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): The model used for tokenization.\n",
    "        inputs (List[str]): The list of input strings to be tokenized.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the following elements:\n",
    "            - tokens (torch.Tensor): The tokenized inputs.\n",
    "            - attention_mask (torch.Tensor): The attention mask for the tokenized inputs.\n",
    "            - input_lengths (torch.Tensor): The lengths of the tokenized inputs.\n",
    "            - n_pos (int): The maximum sequence length of the tokenized inputs.\n",
    "    \"\"\"\n",
    "    if max_length is not None:\n",
    "        old_n_ctx = model.config.n_ctx\n",
    "        model.config.n_ctx = max_length\n",
    "\n",
    "\n",
    "    # tokens = model.to_tokens(inputs, prepend_bos=True, padding_side='right', truncate=(max_length is not None))\n",
    "    # Shun's change\n",
    "    tokenizer = model.tokenizer\n",
    "    tokens = to_tokens(tokenizer, model, inputs, prepend_bos=True, padding_side='right', truncate=(max_length is not None))\n",
    "    \n",
    "    \n",
    "    if max_length is not None:\n",
    "        model.config.n_ctx = old_n_ctx\n",
    "    attention_mask = get_attention_mask(model.tokenizer, tokens, True)\n",
    "    input_lengths = attention_mask.sum(1)\n",
    "    n_pos = attention_mask.size(1)\n",
    "    return tokens, attention_mask, input_lengths, n_pos\n",
    "\n",
    "\n",
    "def make_hooks_and_matrices(model: HookedTransformer, graph: Graph, batch_size:int , n_pos:int, scores: Optional[Tensor]):\n",
    "    \"\"\"Makes a matrix, and hooks to fill it and the score matrix up\n",
    "\n",
    "    Args:\n",
    "        model (HookedTransformer): model to attribute\n",
    "        graph (Graph): graph to attribute\n",
    "        batch_size (int): size of the particular batch you're attributing\n",
    "        n_pos (int): size of the position dimension\n",
    "        scores (Tensor): The scores tensor you intend to fill. If you pass in None, we assume that you're using these hooks / matrices for evaluation only (so don't use the backwards hooks!)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Tuple[List, List, List], Tensor]: The final tensor ([batch, pos, n_src_nodes, d_model]) stores activation differences, i.e. corrupted - clean activations. The first set of hooks will add in the activations they are run on (run these on corrupted input), while the second set will subtract out the activations they are run on (run these on clean input). The third set of hooks will compute the gradients and update the scores matrix that you passed in. \n",
    "    \"\"\"\n",
    "    separate_activations = model.config.use_normalization_before_and_after and scores is None\n",
    "    if separate_activations:\n",
    "        activation_difference = torch.zeros((2, batch_size, n_pos, graph.n_forward, model.config.d_model), device=model.config.device, dtype=model.config.dtype)\n",
    "    else:\n",
    "        # activation_difference = torch.zeros((batch_size, n_pos, graph.n_forward, model.config.d_model), device=model.config.device, dtype=model.config.dtype)\n",
    "        activation_difference = torch.zeros(\n",
    "            (batch_size, n_pos, graph.n_forward, model.config.n_embd),  # using n_embd instead of d_model\n",
    "            device=model.device,  # device from model instead of config\n",
    "            dtype=model.dtype    # dtype from model instead of config\n",
    "        )\n",
    "\n",
    "    processed_attn_layers = set()\n",
    "    fwd_hooks_clean = []\n",
    "    fwd_hooks_corrupted = []\n",
    "    bwd_hooks = []\n",
    "        \n",
    "    # Fills up the activation difference matrix. In the default case (not separate_activations), \n",
    "    # we add in the corrupted activations (add = True) and subtract out the clean ones (add=False)\n",
    "    # In the separate_activations case, we just store them in two halves of the matrix. Less efficient, \n",
    "    # but necessary for models with Gemma's architecture.\n",
    "    def activation_hook(index, activations, hook, add:bool=True):\n",
    "        acts = activations.detach()\n",
    "        try:\n",
    "            if separate_activations:\n",
    "                if add:\n",
    "                    activation_difference[0, :, :, index] += acts\n",
    "                else:\n",
    "                    activation_difference[1, :, :, index] += acts\n",
    "            else:\n",
    "                if add:\n",
    "                    activation_difference[:, :, index] += acts\n",
    "                else:\n",
    "                    activation_difference[:, :, index] -= acts\n",
    "        except RuntimeError as e:\n",
    "            print(hook.name, activation_difference[:, :, index].size(), acts.size())\n",
    "            raise e\n",
    "    \n",
    "    def gradient_hook(prev_index: int, bwd_index: Union[slice, int], gradients:torch.Tensor, hook):\n",
    "        \"\"\"Takes in a gradient and uses it and activation_difference \n",
    "        to compute an update to the score matrix\n",
    "\n",
    "        Args:\n",
    "            fwd_index (Union[slice, int]): The forward index of the (src) node\n",
    "            bwd_index (Union[slice, int]): The backward index of the (dst) node\n",
    "            gradients (torch.Tensor): The gradients of this backward pass \n",
    "            hook (_type_): (unused)\n",
    "\n",
    "        \"\"\"\n",
    "        grads = gradients.detach()\n",
    "        try:\n",
    "            if grads.ndim == 3:\n",
    "                grads = grads.unsqueeze(2)\n",
    "            s = einsum(activation_difference[:, :, :prev_index], grads,'batch pos forward hidden, batch pos backward hidden -> forward backward')\n",
    "            s = s.squeeze(1)\n",
    "            scores[:prev_index, bwd_index] += s\n",
    "        except RuntimeError as e:\n",
    "            print(hook.name, activation_difference.size(), activation_difference.device, grads.size(), grads.device)\n",
    "            print(prev_index, bwd_index, scores.size(), s.size())\n",
    "            raise e\n",
    "    \n",
    "    node = graph.nodes['input']\n",
    "    fwd_index = graph.forward_index(node)\n",
    "    fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "    fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "    \n",
    "    for layer in range(graph.cfg['n_layers']):\n",
    "        node = graph.nodes[f'a{layer}.h0']\n",
    "        fwd_index = graph.forward_index(node)\n",
    "        fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "        fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "        prev_index = graph.prev_index(node)\n",
    "        for i, letter in enumerate('qkv'):\n",
    "            bwd_index = graph.backward_index(node, qkv=letter)\n",
    "            bwd_hooks.append((node.qkv_inputs[i], partial(gradient_hook, prev_index, bwd_index)))\n",
    "\n",
    "        node = graph.nodes[f'm{layer}']\n",
    "        fwd_index = graph.forward_index(node)\n",
    "        bwd_index = graph.backward_index(node)\n",
    "        prev_index = graph.prev_index(node)\n",
    "        fwd_hooks_corrupted.append((node.out_hook, partial(activation_hook, fwd_index)))\n",
    "        fwd_hooks_clean.append((node.out_hook, partial(activation_hook, fwd_index, add=False)))\n",
    "        bwd_hooks.append((node.in_hook, partial(gradient_hook, prev_index, bwd_index)))\n",
    "        \n",
    "    node = graph.nodes['logits']\n",
    "    prev_index = graph.prev_index(node)\n",
    "    bwd_index = graph.backward_index(node)\n",
    "    bwd_hooks.append((node.in_hook, partial(gradient_hook, prev_index, bwd_index)))\n",
    "            \n",
    "    return (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks), activation_difference\n",
    "\n",
    "\n",
    "# def get_scores_eap_nnsight(model: HookedTransformer, graph: Graph, dataloader:DataLoader, metric: Callable[[Tensor], Tensor], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "#     \"\"\"Gets edge attribution scores using EAP.\n",
    "\n",
    "#     Args:\n",
    "#         model (HookedTransformer): The model to attribute\n",
    "#         graph (Graph): Graph to attribute\n",
    "#         dataloader (DataLoader): The data over which to attribute\n",
    "#         metric (Callable[[Tensor], Tensor]): metric to attribute with respect to\n",
    "#         quiet (bool, optional): suppress tqdm output. Defaults to False.\n",
    "\n",
    "#     Returns:\n",
    "#         Tensor: a [src_nodes, dst_nodes] tensor of scores for each edge\n",
    "#     \"\"\"\n",
    "#     # scores = torch.zeros((graph.n_forward, graph.n_backward), device='cuda', dtype=model.config.dtype)\n",
    "#     scores = torch.zeros((graph.n_forward, graph.n_backward), device='cpu', dtype=model.config.dtype)\n",
    "\n",
    "#     if 'mean' in intervention:\n",
    "#         assert intervention_dataloader is not None, \"Intervention dataloader must be provided for mean interventions\"\n",
    "    #     per_position = 'positional' in intervention\n",
    "    #     means = compute_mean_activations(model, graph, intervention_dataloader, per_position=per_position)\n",
    "    #     means = means.unsqueeze(0)\n",
    "    #     if not per_position:\n",
    "    #         means = means.unsqueeze(0)\n",
    "    \n",
    "    # total_items = 0\n",
    "    # dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    # for clean, corrupted, label in dataloader:\n",
    "    #     batch_size = len(clean)\n",
    "    #     total_items += batch_size\n",
    "    #     clean_tokens, attention_mask, input_lengths, n_pos = tokenize_plus_nnsight(model, clean)\n",
    "    #     corrupted_tokens, _, _, _ = tokenize_plus_nnsight(model, corrupted)\n",
    "\n",
    "    #     (fwd_hooks_corrupted, fwd_hooks_clean, bwd_hooks), activation_difference = make_hooks_and_matrices(model, graph, batch_size, n_pos, scores)\n",
    "\n",
    "    #     with torch.inference_mode():\n",
    "    #         if intervention == 'patching':\n",
    "    #             # We intervene by subtracting out clean and adding in corrupted activations\n",
    "    #             with model.hooks(fwd_hooks_corrupted):\n",
    "    #                 _ = model(corrupted_tokens, attention_mask=attention_mask)\n",
    "                    \n",
    "                    \n",
    "    #         elif 'mean' in intervention:\n",
    "    #             # In the case of zero or mean ablation, we skip the adding in corrupted activations\n",
    "    #             # but in mean ablations, we need to add the mean in\n",
    "    #             activation_difference += means\n",
    "\n",
    "    #         # For some metrics (e.g. accuracy or KL), we need the clean logits\n",
    "    #         clean_logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "\n",
    "    #     with model.hooks(fwd_hooks=fwd_hooks_clean, bwd_hooks=bwd_hooks):\n",
    "    #         logits = model(clean_tokens, attention_mask=attention_mask)\n",
    "    #         metric_value = metric(logits, clean_logits, input_lengths, label)\n",
    "    #         metric_value.backward()\n",
    "\n",
    "    # scores /= total_items\n",
    "\n",
    "    # return scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_scores_eap_nnsight(model: LanguageModel, graph: Graph, dataloader: DataLoader, metric: Callable[[Tensor], Tensor], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "    scores = torch.zeros((graph.n_forward, graph.n_backward), device='cpu', dtype=model.dtype)\n",
    "\n",
    "    total_items = 0\n",
    "    dataloader = dataloader if quiet else tqdm(dataloader)\n",
    "    for clean, corrupted, label in dataloader:\n",
    "        batch_size = len(clean)\n",
    "        total_items += batch_size\n",
    "        clean_tokens, attention_mask, input_lengths, n_pos = tokenize_plus_nnsight(model, clean)\n",
    "        corrupted_tokens, _, _, _ = tokenize_plus_nnsight(model, corrupted)\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            if intervention == 'patching':\n",
    "                activation_difference = torch.zeros(\n",
    "                    (batch_size, n_pos, graph.n_forward, model.config.hidden_size), \n",
    "                    device=model.device, \n",
    "                    dtype=model.dtype\n",
    "                )\n",
    "\n",
    "                # Get clean logits first using trace=False\n",
    "                # clean_logits = model.trace({'input_ids': clean_tokens, 'attention_mask':attention_mask}, trace=False)['logits']\n",
    "                \n",
    "\n",
    "                # Capture corrupted activations\n",
    "                with model.trace({\"input_ids\": corrupted_tokens, \"attention_mask\": attention_mask}) as trace:\n",
    "                    for layer in range(graph.cfg['n_layers']):\n",
    "                        node = graph.nodes[f'a{layer}.h0']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        attn_hs = model.transformer.h[layer].attn.c_proj.input\n",
    "\n",
    "                        by_head = split_heads_nns(attn_hs, model.config.n_head, model.config.hidden_size)\n",
    "\n",
    "                        by_head = model.transformer.h[layer].attn.c_proj(by_head)\n",
    "                        by_head = model.transformer.h[layer].attn.resid_dropout(by_head)\n",
    "                        \n",
    "                        activation_difference[:, :, fwd_index] += by_head\n",
    "\n",
    "                        node = graph.nodes[f'm{layer}']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        activation_difference[:, :, fwd_index][:] += model.transformer.h[layer].mlp.output[:]\n",
    "\n",
    "            elif 'mean' in intervention:\n",
    "                activation_difference += means\n",
    "\n",
    "            # Run with input modifications\n",
    "            with model.trace({\"input_ids\": clean_tokens, \"attention_mask\": attention_mask}) as trace:\n",
    "                for layer in range(graph.cfg['n_layers']):\n",
    "                    if any(graph.nodes[f'a{layer}.h{head}'].in_graph for head in range(model.config.n_head)):\n",
    "                        # Get layer input\n",
    "                        qkv_inp = model.transformer.h[layer].ln_1.input\n",
    "                        node = graph.nodes[f'a{layer}.h0']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        \n",
    "                        update = activation_difference[:, :, fwd_index]\n",
    "                        model.transformer.h[layer].attn.output[:] += update\n",
    "\n",
    "                    # MLP handling\n",
    "                    if graph.nodes[f'm{layer}'].in_graph:\n",
    "                        node = graph.nodes[f'm{layer}']\n",
    "                        fwd_index = graph.forward_index(node)\n",
    "                        model.transformer.h[layer].mlp.output[:] += activation_difference[:, :, fwd_index]\n",
    "\n",
    "                logits = model.lm_head.output.save()\n",
    "                metric_value = metric(logits, clean_logits, input_lengths, label)\n",
    "                metric_value.backward()\n",
    "\n",
    "                # Collect gradients\n",
    "                grads = model.lm_head.output.grad\n",
    "                s = einsum(activation_difference, grads,\n",
    "                          'batch pos forward hidden, batch pos token -> forward')\n",
    "                scores += s\n",
    "\n",
    "    scores /= total_items\n",
    "    return scores\n",
    "\n",
    "allowed_aggregations = {'sum', 'mean'}#, 'l2'}        \n",
    "def attribute_nnsight(model: LanguageModel, graph: Graph, dataloader: DataLoader, metric: Callable[[Tensor], Tensor], method: Literal['EAP', 'EAP-IG-inputs', 'clean-corrupted', 'EAP-IG-activations'], intervention: Literal['patching', 'zero', 'mean','mean-positional']='patching', aggregation='sum', ig_steps: Optional[int]=None, intervention_dataloader: Optional[DataLoader]=None, quiet=False):\n",
    "    assert model.config.use_attn_result, \"Model must be configured to use attention result (model.config.use_attn_result)\"\n",
    "    assert model.config.use_split_qkv_input, \"Model must be configured to use split qkv inputs (model.config.use_split_qkv_input)\"\n",
    "    assert model.config.use_hook_mlp_in, \"Model must be configured to use hook MLP in (model.config.use_hook_mlp_in)\"\n",
    "    if model.config.n_key_value_heads is not None:\n",
    "        assert model.config.ungroup_grouped_query_attention, \"Model must be configured to ungroup grouped attention (model.config.ungroup_grouped_query_attention = True)\"\n",
    "    \n",
    "    if aggregation not in allowed_aggregations:\n",
    "        raise ValueError(f'aggregation must be in {allowed_aggregations}, but got {aggregation}')\n",
    "        \n",
    "    # Scores are by default summed across the d_model dimension\n",
    "    # This means that scores are a [n_src_nodes, n_dst_nodes] tensor\n",
    "    if method == 'EAP':\n",
    "        scores = get_scores_eap_nnsight(model, graph, dataloader, metric, intervention=intervention, intervention_dataloader=intervention_dataloader, quiet=quiet)\n",
    "    elif method == 'EAP-IG-inputs':\n",
    "        if intervention != 'patching':\n",
    "            raise ValueError(f\"intervention must be 'patching' for EAP-IG-inputs, but got {intervention}\")\n",
    "        scores = get_scores_eap_ig(model, graph, dataloader, metric, steps=ig_steps, quiet=quiet)\n",
    "    elif method == 'clean-corrupted':\n",
    "        if intervention != 'patching':\n",
    "            raise ValueError(f\"intervention must be 'patching' for clean-corrupted, but got {intervention}\")\n",
    "        scores = get_scores_clean_corrupted(model, graph, dataloader, metric, quiet=quiet)\n",
    "    elif method == 'EAP-IG-activations':\n",
    "        scores = get_scores_ig_activations(model, graph, dataloader, metric, steps=ig_steps, intervention=intervention, intervention_dataloader=intervention_dataloader, quiet=quiet)\n",
    "    else:\n",
    "        raise ValueError(f\"integrated_gradients must be in ['EAP', 'EAP-IG-inputs', 'EAP-IG-activations'], but got {method}\")\n",
    "\n",
    "\n",
    "    if aggregation == 'mean':\n",
    "        scores /= model.config.d_model\n",
    "        \n",
    "    graph.scores[:] =  scores.to(graph.scores.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel('gpt2', device_map='cpu')\n",
    "model.config.use_split_qkv_input = True\n",
    "model.config.use_attn_result = True\n",
    "model.config.use_hook_mlp_in = True\n",
    "model.config.ungroup_grouped_query_attention = True\n",
    "\n",
    "dataset = HFEAPDataset(\"danaarad/ioi_dataset\", model.tokenizer, task=\"ioi\", num_examples=100)\n",
    "dataloader = dataset.to_dataloader(20)\n",
    "metric_fn = get_metric(\"logit_diff\", \"ioi\", model.tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"_name_or_path\": \"gpt2\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 12,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 1024,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.47.0\",\n",
       "  \"ungroup_grouped_query_attention\": true,\n",
       "  \"use_attn_result\": true,\n",
       "  \"use_cache\": true,\n",
       "  \"use_hook_mlp_in\": true,\n",
       "  \"use_split_qkv_input\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.n_key_value_heads = None\n",
    "model.config.dtype = torch.float32\n",
    "model.config.use_normalization_before_and_after = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): WrapperModule()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Graph.from_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "\n",
      " 0%|                                                                                                 | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Cannot copy out of meta tensor; no data!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_normalization_before_and_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     15\u001b[0m g \u001b[38;5;241m=\u001b[39m Graph\u001b[38;5;241m.\u001b[39mfrom_model(model)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mattribute_nnsight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEAP\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 483\u001b[0m, in \u001b[0;36mattribute_nnsight\u001b[0;34m(model, graph, dataloader, metric, method, intervention, aggregation, ig_steps, intervention_dataloader, quiet)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;66;03m# Scores are by default summed across the d_model dimension\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;66;03m# This means that scores are a [n_src_nodes, n_dst_nodes] tensor\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEAP\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 483\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mget_scores_eap_nnsight\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintervention_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintervention_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEAP-IG-inputs\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m intervention \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpatching\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "Cell \u001b[0;32mIn[1], line 415\u001b[0m, in \u001b[0;36mget_scores_eap_nnsight\u001b[0;34m(model, graph, dataloader, metric, intervention, intervention_dataloader, quiet)\u001b[0m\n\u001b[1;32m    408\u001b[0m activation_difference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    409\u001b[0m     (batch_size, n_pos, graph\u001b[38;5;241m.\u001b[39mn_forward, model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mhidden_size), \n\u001b[1;32m    410\u001b[0m     device\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice, \n\u001b[1;32m    411\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    412\u001b[0m )\n\u001b[1;32m    414\u001b[0m \u001b[38;5;66;03m# Get clean logits first using trace=False\u001b[39;00m\n\u001b[0;32m--> 415\u001b[0m clean_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mclean_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    417\u001b[0m \u001b[38;5;66;03m# Capture corrupted activations\u001b[39;00m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: corrupted_tokens, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask}) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:267\u001b[0m, in \u001b[0;36mNNsight.trace\u001b[0;34m(self, trace, invoker_args, backend, remote, blocking, scan, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer:\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvoker_args):\n\u001b[0;32m--> 267\u001b[0m             output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mvalue\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Otherwise open an invoker context with the give args.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/contexts/Tracer.py:102\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minvoker\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__exit__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mexc_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc_tb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/contexts/GraphBasedContext.py:215\u001b[0m, in \u001b[0;36mGraphBasedContext.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39malive \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/models/NNsightModel.py:265\u001b[0m, in \u001b[0;36mNNsight.trace\u001b[0;34m(self, trace, invoker_args, backend, remote, blocking, scan, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trace:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tracer:\n\u001b[0;32m--> 265\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tracer\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minvoker_args):\n\u001b[1;32m    267\u001b[0m             output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/contexts/Invoker.py:125\u001b[0m, in \u001b[0;36mInvoker.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39m_envoy\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtracer\u001b[38;5;241m.\u001b[39m_invoker_inputs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/contexts/GraphBasedContext.py:349\u001b[0m, in \u001b[0;36mGlobalTracingContext.GlobalTracingExit.__exit__\u001b[0;34m(self, exc_type, exc_val, traceback)\u001b[0m\n\u001b[1;32m    345\u001b[0m GlobalTracingContext\u001b[38;5;241m.\u001b[39mPATCHER\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__enter__\u001b[39m()\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 349\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/contexts/Invoker.py:82\u001b[0m, in \u001b[0;36mInvoker.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m GlobalTracingContext\u001b[38;5;241m.\u001b[39mexit_global_tracing_context():\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_proxies_in_inputs:\n\u001b[0;32m---> 82\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtracer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan:\n\u001b[1;32m     88\u001b[0m         inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/models/LanguageModel.py:262\u001b[0m, in \u001b[0;36mLanguageModel._prepare_inputs\u001b[0;34m(self, inputs, labels, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    260\u001b[0m     new_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 262\u001b[0m     tokenized_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m     new_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/nnsight/models/LanguageModel.py:239\u001b[0m, in \u001b[0;36mLanguageModel._tokenize\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    238\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: ids} \u001b[38;5;28;01mfor\u001b[39;00m ids \u001b[38;5;129;01min\u001b[39;00m inputs]\n\u001b[0;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(inputs, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3335\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   3329\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3330\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfirst_element\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unknown: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_element)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of a python, numpy, pytorch or tensorflow object.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3332\u001b[0m         )\n\u001b[1;32m   3334\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m encoded_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m-> 3335\u001b[0m         encoded_inputs[key] \u001b[38;5;241m=\u001b[39m \u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3337\u001b[0m \u001b[38;5;66;03m# Convert padding_strategy in PaddingStrategy\u001b[39;00m\n\u001b[1;32m   3338\u001b[0m padding_strategy, _, max_length, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3339\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding, max_length\u001b[38;5;241m=\u001b[39mmax_length, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[1;32m   3340\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/transformers/utils/generic.py:269\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [to_py_obj(o) \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/transformers/utils/generic.py:269\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mto_py_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m obj]\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# This gives us a smart order to test the frameworks with the corresponding tests.\u001b[39;00m\n\u001b[1;32m    272\u001b[0m framework_to_test_func \u001b[38;5;241m=\u001b[39m _get_frameworks_and_test_func(obj)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/transformers/utils/generic.py:275\u001b[0m, in \u001b[0;36mto_py_obj\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m framework, test_func \u001b[38;5;129;01min\u001b[39;00m framework_to_test_func\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m test_func(obj):\n\u001b[0;32m--> 275\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mframework_to_py_obj\u001b[49m\u001b[43m[\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;66;03m# tolist also works on 0d np arrays\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, np\u001b[38;5;241m.\u001b[39mnumber):\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mib/lib/python3.9/site-packages/transformers/utils/generic.py:260\u001b[0m, in \u001b[0;36mto_py_obj.<locals>.<lambda>\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mto_py_obj\u001b[39m(obj):\n\u001b[1;32m    255\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;124;03m    Convert a TensorFlow tensor, PyTorch tensor, Numpy array or python list to a python list.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    259\u001b[0m     framework_to_py_obj \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    262\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: np\u001b[38;5;241m.\u001b[39masarray(obj)\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    263\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m obj: obj\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[1;32m    264\u001b[0m     }\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (\u001b[38;5;28mdict\u001b[39m, UserDict)):\n\u001b[1;32m    267\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {k: to_py_obj(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mitems()}\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Cannot copy out of meta tensor; no data!"
     ]
    }
   ],
   "source": [
    "model = LanguageModel('gpt2', device_map='cpu')\n",
    "model.config.use_split_qkv_input = True\n",
    "model.config.use_attn_result = True\n",
    "model.config.use_hook_mlp_in = True\n",
    "model.config.ungroup_grouped_query_attention = True\n",
    "\n",
    "dataset = HFEAPDataset(\"danaarad/ioi_dataset\", model.tokenizer, task=\"ioi\", num_examples=100)\n",
    "dataloader = dataset.to_dataloader(20)\n",
    "metric_fn = get_metric(\"logit_diff\", \"ioi\", model.tokenizer, model)\n",
    "\n",
    "model.config.n_key_value_heads = None\n",
    "model.config.dtype = torch.float32\n",
    "model.config.use_normalization_before_and_after = False\n",
    "\n",
    "g = Graph.from_model(model)\n",
    "\n",
    "attribute_nnsight(model, g, dataloader, partial(metric_fn, loss=True, mean=True), 'EAP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight: torch.float32\n",
      "Model dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model\n",
    "import torch\n",
    "\n",
    "# Load the model\n",
    "model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "# Print the dtype of model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.dtype}\")\n",
    "    # You can break after first parameter since they typically share the same dtype\n",
    "    break\n",
    "\n",
    "# Alternative: get dtype of the first parameter directly\n",
    "first_param = next(model.parameters())\n",
    "print(f\"Model dtype: {first_param.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
